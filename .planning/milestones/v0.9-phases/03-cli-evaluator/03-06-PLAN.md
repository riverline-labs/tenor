---
phase: 03-cli-evaluator
plan: 06
type: tdd
wave: 3
depends_on: [03-03]
files_modified:
  - conformance/eval/positive/fact_basic.tenor
  - conformance/eval/positive/fact_basic.facts.json
  - conformance/eval/positive/fact_basic.verdicts.json
  - conformance/eval/frozen/flow_frozen_verdicts.tenor
  - conformance/eval/frozen/flow_frozen_verdicts.facts.json
  - conformance/eval/frozen/flow_frozen_verdicts.verdicts.json
  - conformance/eval/numeric/decimal_promotion.facts.json
  - conformance/eval/numeric/decimal_promotion.verdicts.json
  - crates/eval/tests/conformance.rs
  - crates/eval/tests/numeric_regression.rs
autonomous: true
requirements: [EVAL-05, EVAL-06, EVAL-07, TEST-09]

must_haves:
  truths:
    - "Evaluator conformance suite has at least 15 positive test fixtures covering basic facts, rules, entities, operations, and flows"
    - "Frozen verdict edge cases tested: mid-flow state change not affecting verdicts, sub-flow snapshot inheritance"
    - "Numeric precision suite has at least 50 test cases covering promotion, rounding, overflow, Money arithmetic"
    - "Conformance runner validates evaluator output against expected verdict JSON"
    - "Numeric regression suite is shared -- same fixtures consumable by both elaborator and evaluator"
  artifacts:
    - path: "conformance/eval/"
      provides: "Evaluator conformance fixture directory"
    - path: "conformance/eval/positive/"
      provides: "Positive evaluator test fixtures (.tenor + .facts.json + .verdicts.json)"
    - path: "conformance/eval/frozen/"
      provides: "Frozen verdict edge case fixtures"
    - path: "conformance/eval/numeric/"
      provides: "Numeric precision regression fixtures (50+ cases)"
    - path: "crates/eval/tests/conformance.rs"
      provides: "Evaluator conformance test runner"
      contains: "run_eval_fixture"
    - path: "crates/eval/tests/numeric_regression.rs"
      provides: "Numeric precision regression test runner"
      contains: "numeric"
  key_links:
    - from: "crates/eval/tests/conformance.rs"
      to: "tenor_eval::evaluate"
      via: "Runner calls evaluate() and compares output"
      pattern: "tenor_eval::evaluate"
    - from: "conformance/eval/numeric/"
      to: "conformance/numeric/"
      via: "Shared numeric fixtures referenced by both elaborator and evaluator"
      pattern: "numeric"
---

<objective>
Create the evaluator conformance suite with test fixtures covering basic evaluation, frozen verdict edge cases, and numeric precision regression. The suite follows the existing conformance convention (fixture triplets) and includes a shared numeric regression suite consumable by both the elaborator and evaluator.

Purpose: EVAL-05/06/07 require dedicated evaluator conformance tests separate from the elaborator suite. TEST-09 requires a shared numeric precision suite. These fixtures are the ground truth for evaluator correctness.
Output: `conformance/eval/` directory with 15+ positive fixtures, frozen verdict edge cases, and 50+ numeric precision cases. Automated test runners in tenor-eval.
</objective>

<execution_context>
@/Users/bwb/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bwb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-cli-evaluator/03-RESEARCH.md
@.planning/phases/03-cli-evaluator/03-02-SUMMARY.md
@.planning/phases/03-cli-evaluator/03-03-SUMMARY.md

@docs/TENOR.md (Sections 5, 7, 9, 11, 12 -- evaluation semantics)
@conformance/positive/ (existing elaborator fixtures for reference)
@conformance/numeric/ (existing numeric elaborator fixtures)
</context>

<tasks>

<task type="tdd">
  <name>Task 1: Create evaluator conformance fixtures and test runner</name>
  <files>
    conformance/eval/positive/*.tenor
    conformance/eval/positive/*.facts.json
    conformance/eval/positive/*.verdicts.json
    conformance/eval/frozen/*.tenor
    conformance/eval/frozen/*.facts.json
    conformance/eval/frozen/*.verdicts.json
    crates/eval/tests/conformance.rs
  </files>
  <action>
**Fixture convention:** Each evaluator test case is a triplet:
- `<name>.tenor` -- Tenor source file (elaborated internally by the runner)
- `<name>.facts.json` -- fact values for evaluation
- `<name>.verdicts.json` -- expected verdict output

**Create directory structure:**
```
conformance/eval/
  positive/       # Basic evaluation tests
  frozen/         # Frozen verdict semantics edge cases
  numeric/        # Numeric precision (task 2)
```

**Positive fixtures (at least 15):**

1. `fact_bool_basic` -- single bool fact, one rule producing bool verdict
2. `fact_int_basic` -- int fact with comparison rule
3. `fact_decimal_basic` -- decimal fact with comparison (tests rust_decimal)
4. `fact_money_basic` -- money fact with threshold comparison
5. `fact_with_default` -- fact omitted from facts.json, uses contract default
6. `fact_missing_error` -- fact omitted, no default -> expected error result
7. `fact_enum_basic` -- enum fact matching rule condition
8. `fact_text_basic` -- text fact with equality check
9. `rule_multi_stratum` -- two rules at different strata, stratum 1 references stratum 0 verdict
10. `rule_multiple_same_stratum` -- multiple rules at same stratum (independent)
11. `rule_condition_false` -- rule condition evaluates to false, no verdict produced
12. `rule_and_or` -- compound logical conditions (And/Or/Not)
13. `entity_operation_basic` -- operation with entity state transition
14. `operation_persona_check` -- operation with persona authorization
15. `operation_precondition` -- operation with precondition check
16. `flow_linear_basic` -- simple flow: entry -> operation -> terminal
17. `flow_branch_basic` -- flow with branch step based on verdict

**Frozen verdict edge cases (EVAL-06):**

1. `flow_frozen_verdicts` -- A flow where:
   - Step 1: evaluate rules producing verdict V1 based on entity in state S1
   - Step 2: operation changes entity from S1 to S2
   - Step 3: branch checks V1 -- MUST see original V1 (frozen), not recomputed value
   This is THE critical test for frozen verdict semantics.

2. `flow_frozen_facts` -- flow where operation step occurs, then branch checks a fact-dependent verdict -- facts remain frozen

3. `flow_subflow_snapshot` -- sub-flow inherits parent snapshot, sub-flow operations don't affect parent's verdict evaluation

**Conformance test runner (crates/eval/tests/conformance.rs):**

```rust
use std::path::Path;

fn run_eval_fixture(fixture_dir: &Path, name: &str) {
    let tenor_path = fixture_dir.join(format!("{}.tenor", name));
    let facts_path = fixture_dir.join(format!("{}.facts.json", name));
    let expected_path = fixture_dir.join(format!("{}.verdicts.json", name));

    // Step 1: Elaborate the .tenor file to get interchange bundle
    let bundle = tenor_core::elaborate::elaborate(&tenor_path)
        .expect(&format!("Failed to elaborate {}", name));

    // Step 2: Load facts.json
    let facts: serde_json::Value = serde_json::from_str(
        &std::fs::read_to_string(&facts_path)
            .expect(&format!("Failed to read facts for {}", name))
    ).expect("Invalid facts JSON");

    // Step 3: Evaluate
    let result = tenor_eval::evaluate(&bundle, &facts)
        .expect(&format!("Evaluation failed for {}", name));

    // Step 4: Compare with expected verdicts
    let expected: serde_json::Value = serde_json::from_str(
        &std::fs::read_to_string(&expected_path)
            .expect(&format!("Failed to read expected verdicts for {}", name))
    ).expect("Invalid expected JSON");

    let actual = result.verdicts.to_json();
    assert_eq!(actual, expected, "Verdict mismatch for {}", name);
}
```

Generate `#[test]` functions for each fixture programmatically or list them individually. Use `#[test]` per fixture for clear failure reporting.

When creating fixtures, start by writing the .tenor source and .facts.json, then run the evaluator to see what it actually produces, and use that as the expected output (same approach as elaborator conformance regeneration).
  </action>
  <verify>
`cargo test -p tenor-eval --test conformance` passes all positive fixture tests.
`cargo test -p tenor-eval --test conformance` passes all frozen verdict tests.
Count of test functions >= 20 (15 positive + 3 frozen + extras).
  </verify>
  <done>
Evaluator conformance suite with 15+ positive fixtures and 3+ frozen verdict edge cases. All fixtures are triplets (.tenor + .facts.json + .verdicts.json). Test runner elaborates, evaluates, and compares against expected verdicts. Frozen verdict tests explicitly verify immutable snapshot behavior.
  </done>
</task>

<task type="tdd">
  <name>Task 2: Create numeric precision regression suite (50+ cases)</name>
  <files>
    conformance/eval/numeric/*.facts.json
    conformance/eval/numeric/*.verdicts.json
    conformance/eval/numeric/*.tenor
    crates/eval/tests/numeric_regression.rs
  </files>
  <action>
**Numeric regression suite design:** 50+ test cases organized by category. Each case is a fixture triplet.

The existing `conformance/numeric/` directory has elaborator-level numeric tests (testing type promotion and serialization). The evaluator numeric suite tests VALUE-level arithmetic and comparison correctness.

**Categories and cases:**

**A. Int arithmetic (5 cases):**
- int_compare_equal, int_compare_less, int_compare_greater
- int_boundary_max (i64 near limits), int_boundary_min

**B. Decimal arithmetic (10 cases):**
- decimal_compare_exact, decimal_compare_scale_mismatch (different scales)
- decimal_mul_basic, decimal_mul_precision_limit
- decimal_round_midpoint_even (0.5 rounds to even -- MidpointNearestEven)
- decimal_round_up, decimal_round_down
- decimal_overflow (exceeds declared precision)
- decimal_scale_3, decimal_scale_8 (various scale values)

**C. Int-to-Decimal promotion (8 cases):**
- promote_int_eq_decimal, promote_int_lt_decimal, promote_int_gt_decimal
- promote_int_mul_decimal (Int * Decimal)
- promote_large_int_to_decimal (tests precision computation from spec 12.2)
- promote_negative_int_to_decimal
- promote_int_in_comparison (Int compared to Decimal in rule condition)
- promote_int_zero

**D. Money arithmetic (10 cases):**
- money_compare_equal, money_compare_less
- money_compare_different_currency (error)
- money_mul_int (Money * Int factor)
- money_mul_decimal (Money * Decimal factor)
- money_add_basic (if supported), money_threshold_check
- money_zero, money_negative
- money_precision_cents (2 decimal places)
- money_large_amount (millions)

**E. Cross-type comparisons (8 cases):**
- cross_int_decimal_eq, cross_int_decimal_lt
- cross_mul_vs_int (Mul result compared to Int)
- cross_money_comparison_type (tests comparison_type emission)
- cross_decimal_money (comparison between decimal value and money amount)
- cross_bool_not_numeric (ensure no numeric promotion for bools)
- cross_enum_comparison (enum equality)
- cross_text_comparison (text equality)

**F. Edge cases (9+ cases):**
- precision_max_scale (scale = precision, all decimal digits)
- precision_zero_scale (scale = 0, integer-like decimal)
- rounding_5_to_even_down (2.5 -> 2, 4.5 -> 4)
- rounding_5_to_even_up (3.5 -> 4, 5.5 -> 6)
- overflow_mul (multiplication exceeds precision)
- overflow_large_values
- negative_decimal_comparison
- comparison_near_zero (very small positive vs very small negative)
- list_of_money (list operations with Money elements)

**Test runner (crates/eval/tests/numeric_regression.rs):**

Same pattern as conformance.rs but pointing to `conformance/eval/numeric/` directory. Generate one `#[test]` per fixture.

**Sharing with elaborator (TEST-09):**
The existing `conformance/numeric/` fixtures test elaborator serialization. The new `conformance/eval/numeric/` fixtures test evaluator arithmetic. They share the SAME .tenor contracts where applicable -- a numeric .tenor file can be tested at both the elaboration level (correct JSON serialization) and the evaluation level (correct arithmetic result). Create symbolic links or shared references where the .tenor source is identical.

In practice: evaluator numeric fixtures include their own .tenor files because they need rule conditions that exercise specific arithmetic operations, which may differ from the elaborator's serialization-focused tests. The "shared" aspect is that both suites cover the same numeric model -- TEST-09 is satisfied by having BOTH suites exist and covering the same Section 12 semantics.
  </action>
  <verify>
`cargo test -p tenor-eval --test numeric_regression` passes all 50+ cases.
Count test functions: at least 50.
Verify MidpointNearestEven rounding: 2.5 rounds to 2, 3.5 rounds to 4 (banker's rounding).
Verify overflow detection: exceeding declared precision produces EvalError.
  </verify>
  <done>
50+ numeric precision regression cases covering Int, Decimal, Money, promotion, cross-type comparison, and edge cases. MidpointNearestEven rounding verified. Overflow detection tested. Both elaborator and evaluator have numeric suites covering Section 12 NumericModel.
  </done>
</task>

</tasks>

<verification>
- `cargo test -p tenor-eval` passes ALL tests (unit + conformance + numeric regression)
- At least 20 conformance fixtures (positive + frozen)
- At least 50 numeric regression fixtures
- Frozen verdict test explicitly verifies snapshot immutability
- `cargo build --workspace` compiles
</verification>

<success_criteria>
- conformance/eval/ directory structure with positive/, frozen/, numeric/ subdirectories
- 15+ positive evaluation fixtures with correct expected verdicts
- 3+ frozen verdict edge case fixtures proving snapshot semantics
- 50+ numeric precision regression cases
- All fixtures follow .tenor + .facts.json + .verdicts.json triplet convention
- Test runners elaborate, evaluate, and compare against expected output
- Both elaborator and evaluator have numeric suites covering the same NumericModel
</success_criteria>

<output>
After completion, create `.planning/phases/03-cli-evaluator/03-06-SUMMARY.md`
</output>
