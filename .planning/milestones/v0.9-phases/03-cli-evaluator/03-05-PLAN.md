---
phase: 03-cli-evaluator
plan: 05
type: execute
wave: 3
depends_on: [03-01, 03-03]
files_modified:
  - crates/cli/src/main.rs
  - crates/cli/Cargo.toml
  - crates/cli/tests/cli_integration.rs
autonomous: true
requirements: [CLI-05, TEST-07]

must_haves:
  truths:
    - "`tenor eval <bundle.json> --facts <facts.json>` produces verdict JSON to stdout"
    - "`tenor eval` exits 0 on successful evaluation, 1 on evaluation error"
    - "CLI integration tests verify exit codes, stdout content, and stderr for each subcommand"
    - "Integration tests cover: elaborate, validate, test, eval, diff subcommands"
    - "Error messages are informative: missing files, invalid JSON, evaluation failures"
  artifacts:
    - path: "crates/cli/src/main.rs"
      provides: "Eval subcommand wired to tenor-eval::evaluate"
      contains: "tenor_eval::evaluate"
    - path: "crates/cli/tests/cli_integration.rs"
      provides: "CLI integration tests using assert_cmd"
      contains: "Command::cargo_bin"
  key_links:
    - from: "crates/cli/src/main.rs"
      to: "tenor_eval::evaluate"
      via: "eval subcommand dispatch"
      pattern: "tenor_eval::evaluate"
    - from: "crates/cli/tests/cli_integration.rs"
      to: "tenor"
      via: "assert_cmd spawns binary"
      pattern: "cargo_bin.*tenor"
---

<objective>
Wire the `tenor eval` subcommand to the evaluator library, and create comprehensive CLI integration tests for all implemented subcommands using `assert_cmd`.

Purpose: CLI-05 connects the evaluator to users via the command line. TEST-07 ensures all subcommands have automated integration tests verifying exit codes, output format, and error handling -- critical for CI reliability.
Output: Working `tenor eval` command and integration test suite covering elaborate, validate, eval, test, and diff.
</objective>

<execution_context>
@/Users/bwb/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bwb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-cli-evaluator/03-RESEARCH.md
@.planning/phases/03-cli-evaluator/03-01-SUMMARY.md
@.planning/phases/03-cli-evaluator/03-03-SUMMARY.md

@crates/cli/src/main.rs
@crates/cli/Cargo.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire eval subcommand to tenor-eval library</name>
  <files>
    crates/cli/src/main.rs
    crates/cli/Cargo.toml
  </files>
  <action>
**Cargo.toml:** Add `tenor-eval = { path = "../eval" }` to crates/cli dependencies.

**main.rs eval handler:** Replace the stub `Commands::Eval` handler with:

1. Read bundle JSON from the `bundle` path argument.
2. Read facts JSON from the `--facts` path argument.
3. Parse both as `serde_json::Value`.
4. Call `tenor_eval::evaluate(&bundle, &facts)`.
5. On success:
   - `--output json` (default): print `EvalResult::verdicts.to_json()` to stdout
   - `--output text`: print human-readable verdict summary (verdict type, payload, rule)
   - Exit 0
6. On error:
   - Print error details to stderr (eval error type, message, context)
   - `--output json`: format error as JSON object `{ "error": "...", "details": {...} }`
   - Exit 1
7. `--quiet` mode: suppress output, exit with appropriate code.

**Error handling for file I/O:**
- If bundle file doesn't exist: print "error: bundle file not found: <path>" to stderr, exit 1
- If facts file doesn't exist: print "error: facts file not found: <path>" to stderr, exit 1
- If JSON parsing fails: print "error: invalid JSON in <path>: <parse error>" to stderr, exit 1

Also create a small test fixture for manual verification:
- `tests/fixtures/eval_basic.tenor` -- a minimal contract with 1 fact, 1 rule
- `tests/fixtures/eval_basic.facts.json` -- facts for the basic contract
- `tests/fixtures/eval_basic.expected_verdicts.json` -- expected verdict output

These fixtures go under `crates/cli/tests/fixtures/` for integration tests.

**Elaborate the .tenor fixture to JSON bundle:** After creating `eval_basic.tenor`, run:
```bash
cargo run -p tenor-cli -- elaborate crates/cli/tests/fixtures/eval_basic.tenor > crates/cli/tests/fixtures/eval_basic_bundle.json
```
This produces the interchange bundle JSON that `tenor eval` consumes. Verify the elaboration succeeds before proceeding to manual verification.
  </action>
  <verify>
`cargo build -p tenor-cli` compiles.
Run `tenor eval crates/cli/tests/fixtures/eval_basic_bundle.json --facts crates/cli/tests/fixtures/eval_basic.facts.json` -- produces verdict JSON to stdout, exit 0.
Run `tenor eval nonexistent.json --facts facts.json` -- prints file-not-found error to stderr, exit 1.
  </verify>
  <done>
`tenor eval` reads bundle + facts, calls evaluator, outputs verdicts. File I/O errors handled gracefully. Both JSON and text output formats work. Exit codes correct.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create CLI integration tests with assert_cmd</name>
  <files>
    crates/cli/Cargo.toml
    crates/cli/tests/cli_integration.rs
  </files>
  <action>
**Cargo.toml dev-dependencies:**
```toml
[dev-dependencies]
assert_cmd = "2"
predicates = "3"
tempfile = "3"
```
Add `tempfile` to handle temporary file creation in tests.

**tests/cli_integration.rs:** Create integration tests using `assert_cmd::Command` to spawn the `tenor` binary.

**Test categories:**

1. **Help and version:**
   - `tenor --help` exits 0, stdout contains "Tenor contract language toolchain"
   - `tenor --version` exits 0, stdout contains version number
   - `tenor elaborate --help` exits 0, stdout contains "file"

2. **Elaborate subcommand:**
   - `tenor elaborate conformance/positive/fact_basic.tenor` exits 0, stdout contains `"kind": "Bundle"`
   - `tenor elaborate nonexistent.tenor` exits 1, stderr contains error message
   - `tenor elaborate conformance/negative/duplicate_id.tenor` exits 1 (elaboration error)

3. **Validate subcommand:**
   - `tenor validate <valid-expected-json>` exits 0 (pick a conformance/positive/*.expected.json)
   - `tenor validate <invalid-json>` exits 1 (create a temp file with `{"not": "a bundle"}`)

4. **Test subcommand:**
   - `tenor test conformance` exits 0, stdout contains "55 tests" or "ok" (TAP output)
   - `tenor test nonexistent_dir` exits 1

5. **Eval subcommand:**
   - `tenor eval <bundle> --facts <facts>` with valid fixtures exits 0, stdout contains "verdicts"
   - `tenor eval <bundle> --facts nonexistent.json` exits 1
   - `tenor eval` without --facts flag exits with clap error (missing required argument)

6. **Diff subcommand:**
   - `tenor diff <same> <same>` exits 0 (no differences)
   - `tenor diff <file1> <file2>` with different files exits 1

7. **Stub subcommands:**
   - `tenor check file.tenor` exits 2, stderr contains "not yet implemented"
   - `tenor explain bundle.json` exits 2
   - `tenor generate bundle.json --target typescript` exits 2

8. **Global flags:**
   - `tenor elaborate <file> --quiet` exits 0, stdout is empty or minimal
   - `tenor elaborate <file> --output json` exits 0, produces JSON

**Fixture setup:** For eval tests, first elaborate a .tenor file to get a bundle JSON, then use that as input. Use `tempfile` crate to create temporary files where needed. OR: create small, static fixture files under `crates/cli/tests/fixtures/`.

Prefer static fixture files for reproducibility. Create:
- `crates/cli/tests/fixtures/` directory
- Use existing conformance fixtures where possible (reference by relative path from workspace root)
  </action>
  <verify>
`cargo test -p tenor-cli` passes all integration tests.
`cargo test -p tenor-cli -- --test cli_integration` specifically runs the integration test file.
All subcommand tests exercise exit codes and output content.
  </verify>
  <done>
Integration test suite covers all implemented subcommands (elaborate, validate, test, eval, diff) and stub subcommands (check, explain, generate). Each test verifies exit code, stdout/stderr content. assert_cmd + predicates used for clean assertion patterns.
  </done>
</task>

</tasks>

<verification>
- `cargo build --workspace` compiles
- `cargo test -p tenor-cli` passes all tests (unit + integration)
- `tenor eval` works end-to-end with bundle + facts
- Integration tests cover all 8 subcommands
- Exit codes verified: 0 (success), 1 (error), 2 (not implemented)
</verification>

<success_criteria>
- `tenor eval <bundle> --facts <facts>` produces verdict JSON with provenance
- CLI integration tests exist for every implemented subcommand
- Each test verifies exit code and output content
- Error handling tests verify stderr messages
- Global flag tests verify --quiet and --output behavior
</success_criteria>

<output>
After completion, create `.planning/phases/03-cli-evaluator/03-05-SUMMARY.md`
</output>
