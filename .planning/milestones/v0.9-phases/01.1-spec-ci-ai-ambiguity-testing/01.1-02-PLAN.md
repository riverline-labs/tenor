---
phase: 01.1-spec-ci-ai-ambiguity-testing
plan: 02
type: execute
wave: 2
depends_on: ["01.1-01"]
files_modified:
  - elaborator/src/ambiguity/mod.rs
  - elaborator/src/main.rs
autonomous: true
requirements:
  - SPEC-05

must_haves:
  truths:
    - "Running `tenor-elaborator ambiguity ../conformance` with ANTHROPIC_API_KEY set calls the LLM for each test case, compares verdicts, and prints a TAP report"
    - "Running without ANTHROPIC_API_KEY prints a skip message and exits 0 (not a failure)"
    - "Each LLM disagreement is reported with missing/extra verdicts, LLM reasoning, and confidence level"
    - "The existing 47 conformance tests are not affected by the new subcommand"
    - "Rate limiting is handled gracefully with exponential backoff (no crash on 429)"
  artifacts:
    - path: "elaborator/src/ambiguity/mod.rs"
      provides: "Orchestrator function that runs all ambiguity test cases"
      contains: "fn run_ambiguity_suite"
    - path: "elaborator/src/main.rs"
      provides: "CLI entry point with ambiguity subcommand"
      contains: "\"ambiguity\""
  key_links:
    - from: "elaborator/src/main.rs"
      to: "elaborator/src/ambiguity/mod.rs"
      via: "ambiguity::run_ambiguity_suite call from main match arm"
      pattern: "ambiguity::run_ambiguity_suite"
    - from: "elaborator/src/ambiguity/mod.rs"
      to: "elaborator/src/ambiguity/api.rs"
      via: "call_anthropic invocation per test case"
      pattern: "api::call_anthropic"
    - from: "elaborator/src/ambiguity/mod.rs"
      to: "elaborator/src/ambiguity/fixtures.rs"
      via: "load_test_cases to get all fixtures"
      pattern: "fixtures::load_test_cases"
    - from: "elaborator/src/ambiguity/mod.rs"
      to: "elaborator/src/ambiguity/prompt.rs"
      via: "build_system_prompt and build_user_prompt per test case"
      pattern: "prompt::build_system_prompt"
    - from: "elaborator/src/ambiguity/mod.rs"
      to: "elaborator/src/ambiguity/compare.rs"
      via: "compare_verdicts and parse_llm_response per test case"
      pattern: "compare::compare_verdicts"
    - from: "elaborator/src/ambiguity/mod.rs"
      to: "elaborator/src/ambiguity/report.rs"
      via: "report.add_result then report.print_tap"
      pattern: "report::AmbiguityReport"
---

<objective>
Wire all ambiguity harness modules into an orchestrator and expose it as the `ambiguity` subcommand on the elaborator CLI. This plan connects the fixture loader, prompt builder, API client, comparator, and reporter into an end-to-end pipeline, then integrates it into `main.rs`.

Purpose: Makes the ambiguity harness runnable from the command line, producing a TAP report that identifies spec ambiguity signals.

Output: A working `tenor-elaborator ambiguity <suite-dir>` command that loads test cases, calls the Anthropic API, compares verdicts, and reports findings.
</objective>

<execution_context>
@/Users/bwb/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bwb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01.1-spec-ci-ai-ambiguity-testing/01.1-RESEARCH.md
@.planning/phases/01.1-spec-ci-ai-ambiguity-testing/01.1-01-SUMMARY.md
@elaborator/src/main.rs
@elaborator/src/ambiguity/mod.rs
@elaborator/src/ambiguity/api.rs
@elaborator/src/ambiguity/prompt.rs
@elaborator/src/ambiguity/compare.rs
@elaborator/src/ambiguity/report.rs
@elaborator/src/ambiguity/fixtures.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build orchestrator and wire CLI subcommand</name>
  <files>
    elaborator/src/ambiguity/mod.rs
    elaborator/src/main.rs
  </files>
  <action>
**elaborator/src/ambiguity/mod.rs -- add orchestrator function:**

Add `pub fn run_ambiguity_suite(suite_dir: &Path, spec_path: &Path) -> RunResult` to mod.rs (alongside the existing type definitions from Plan 01). This function is the main entry point:

1. **Check for API key:** Read `ANTHROPIC_API_KEY` from environment. If missing, print `"# Skipping ambiguity tests: ANTHROPIC_API_KEY not set"` to stderr and return `RunResult { failed: 0 }` (not a failure -- just skip).

2. **Load test cases:** Call `fixtures::load_test_cases(suite_dir.join("ambiguity"), suite_dir)`. If the ambiguity directory doesn't exist or has no test cases, print `"# No ambiguity test cases found in {suite_dir}/ambiguity"` and return success.

3. **Build system prompt:** Call `prompt::build_system_prompt(spec_path)`. The spec path should be derived relative to suite_dir (e.g., `suite_dir.parent().unwrap().join("docs/TENOR.md")`) or passed explicitly. Handle error if spec file not found.

4. **Initialize report:** Create `report::AmbiguityReport::new()`.

5. **For each test case:**
   a. Build user prompt: `prompt::build_user_prompt(&test.contract_source, &test.facts)`
   b. Call API with retry: `api::with_retry(|| api::call_anthropic(&api_key, &system_prompt, &user_prompt, "claude-sonnet-4-5-20250514"), 3)`
   c. Parse LLM response: `compare::parse_llm_response(&response_text)`
   d. Compare verdicts: `compare::compare_verdicts(&test.expected_verdicts, &llm_response.verdicts_produced)`
   e. Build AmbiguityResult from comparison + LLM metadata
   f. Add to report: `report.add_result(result)`
   g. Print progress to stderr: `eprintln!("# [{}/{}] {} -- {}", i+1, total, test.name, if match { "match" } else { "MISMATCH" })`

6. **Print TAP report:** `report.print_tap()` to stdout.

7. **Return result:** `RunResult { failed: report.mismatch_count() }`. Note: For CI purposes, mismatches are informational, not failures. Consider adding a `--strict` flag later. For now, return the count but let the caller decide exit code policy.

Accept optional `--model <model>` parameter to override the default model. Accept optional `--spec <path>` to override spec path.

**elaborator/src/main.rs -- add ambiguity subcommand:**

1. Add `mod ambiguity;` declaration alongside existing module declarations.

2. In the `match args[1].as_str()` block, add a new arm:
```rust
"ambiguity" => {
    let suite_dir = if args.len() >= 3 {
        Path::new(&args[2]).to_path_buf()
    } else {
        Path::new("../conformance").to_path_buf()
    };
    let spec_path = if args.len() >= 4 {
        Path::new(&args[3]).to_path_buf()
    } else {
        suite_dir.parent()
            .unwrap_or(Path::new(".."))
            .join("docs/TENOR.md")
    };

    if !suite_dir.exists() {
        eprintln!("error: conformance suite directory not found: {}", suite_dir.display());
        process::exit(1);
    }

    let result = ambiguity::run_ambiguity_suite(&suite_dir, &spec_path);
    // Ambiguity mismatches are informational, not build failures.
    // Exit 0 even with mismatches. Exit 1 only on hard errors (API failure, missing files).
    if result.hard_errors > 0 {
        process::exit(1);
    }
}
```

3. Update the usage message to include the ambiguity subcommand:
```
eprintln!("  tenor-elaborator ambiguity <conformance-suite-dir> [spec-path]");
```

**Adjust RunResult in mod.rs** to distinguish hard errors from ambiguity signals:
```rust
pub struct AmbiguityRunResult {
    pub total: usize,
    pub matches: usize,
    pub mismatches: usize,
    pub hard_errors: usize,  // API failures, missing files, etc.
}
```

Hard errors are: API key present but all retries exhausted, fixture file missing or unparseable, spec file not found. Mismatches (LLM disagrees with ground truth) are NOT hard errors.
  </action>
  <verify>
1. `cd /Users/bwb/src/rll/tenor/elaborator && cargo build` compiles cleanly.
2. `cd /Users/bwb/src/rll/tenor/elaborator && cargo run -- run ../conformance` still reports 47/47 passing (existing tests not broken).
3. `cd /Users/bwb/src/rll/tenor/elaborator && cargo run -- ambiguity ../conformance` without ANTHROPIC_API_KEY set prints skip message and exits 0.
4. `cd /Users/bwb/src/rll/tenor/elaborator && cargo run --` prints updated usage including ambiguity subcommand.
  </verify>
  <done>
The `ambiguity` subcommand is wired end-to-end. Without API key, it skips gracefully. With API key, it loads fixtures, builds prompts, calls the Anthropic API with retry, compares verdict sets, and prints a TAP report. Hard errors (API failures, missing files) cause exit 1. LLM disagreements are informational (exit 0). Existing conformance suite is unaffected.
  </done>
</task>

</tasks>

<verification>
**Without API key (CI default):**
```bash
cd elaborator
unset ANTHROPIC_API_KEY
cargo run -- ambiguity ../conformance
# Expected: prints "# Skipping ambiguity tests: ANTHROPIC_API_KEY not set", exits 0
echo $?  # 0
```

**With API key (manual/local run):**
```bash
cd elaborator
export ANTHROPIC_API_KEY="sk-..."
cargo run -- ambiguity ../conformance
# Expected: TAP output showing test results for 8 test cases
# Each test prints ok/not ok with diagnostic metadata for mismatches
```

**Existing tests unchanged:**
```bash
cd elaborator
cargo run -- run ../conformance
# Expected: 47/47 passing (same as before)
```

**Build verification:**
```bash
cd elaborator
cargo build 2>&1 | grep -c warning  # Should be 0
cargo tree | grep tokio              # Should be empty
```
</verification>

<success_criteria>
The AI ambiguity testing harness is fully operational as `tenor-elaborator ambiguity <suite-dir>`. It loads 8 test cases across 3 contracts, calls the Anthropic API for each, compares LLM verdicts against manually-derived ground truth, and reports findings in TAP format. The harness is opt-in (skips when API key is absent), handles rate limiting with exponential backoff, and distinguishes spec ambiguity signals from hard errors. The existing conformance suite is unaffected.
</success_criteria>

<output>
After completion, create `.planning/phases/01.1-spec-ci-ai-ambiguity-testing/01.1-02-SUMMARY.md`
</output>
