# Phase 01.1: Spec CI -- AI Ambiguity Testing - Research

**Researched:** 2026-02-21
**Domain:** LLM-as-evaluator CI harness for specification ambiguity detection (Rust)
**Confidence:** MEDIUM

## Summary

Phase 01.1 is an inserted phase that builds a Rust-based CI harness which feeds the frozen v1.0 Tenor spec and sample contracts to a frontier LLM, asks it to evaluate rules against sample facts, and compares the LLM's verdicts against ground truth from the elaborator's conformance suite. Disagreement between the LLM and ground truth is treated as a spec ambiguity signal -- meaning the spec is unclear enough that a sophisticated reader interprets it differently than the reference implementation.

The core technical challenge is straightforward HTTP-level: construct a prompt containing the relevant spec sections + a contract + a fact set, call the Anthropic Messages API, parse the structured JSON response, and diff the LLM's verdict set against the expected output. The harder design challenge is crafting prompts that isolate spec interpretation from general LLM reasoning -- we want to detect places where the spec text is genuinely ambiguous, not places where the LLM makes arithmetic errors or hallucinates.

**Primary recommendation:** Use `ureq` (synchronous, no-async, minimal-dependency HTTP client) for Anthropic API calls via direct HTTP. Build the harness as a new binary target in the existing `elaborator` crate (or a sibling crate in a future workspace). Derive ground truth test cases from existing conformance fixtures by pairing `.tenor` files with synthetic fact sets and the expected elaborator output. Use Anthropic's structured outputs beta for reliable JSON parsing of LLM responses.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| `ureq` | 3.x | Synchronous HTTP client for Anthropic API | No async runtime required; pure blocking I/O; minimal dependencies; aligns with project "no async runtime" constraint |
| `serde` | 1.x | JSON serialization/deserialization | Already in project; needed for API request/response bodies |
| `serde_json` | 1.x | JSON parsing | Already in project; needed for structured output parsing and ground truth comparison |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| `base64` | 0.22.x | Encoding if needed for file content | Only if spec text needs base64 encoding in prompts (unlikely) |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| `ureq` | `reqwest` with `blocking` feature | reqwest is heavier (pulls in tokio internally even for blocking); ureq is purpose-built for sync-only use and has fewer dependencies |
| `ureq` | `anthropic-sdk-rust` (community crate) | Community SDKs are async-first, pull in tokio, add large dependency trees; the API surface we need is trivial (one POST endpoint) |
| Direct HTTP | Any Anthropic Rust SDK | All community SDKs (`anthropic-sdk-rust`, `anthropic-rs`, etc.) are unofficial, async-first, and add unnecessary complexity. The Messages API is a single POST with 3 headers -- direct HTTP is simpler and more maintainable |

**Installation (Cargo.toml additions):**
```toml
[dependencies]
ureq = { version = "3", features = ["json"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
```

## Architecture Patterns

### Recommended Project Structure
```
elaborator/src/
  main.rs              # Existing: adds "ambiguity" subcommand
  ambiguity/
    mod.rs             # Orchestrator: load fixtures, run prompts, compare results
    api.rs             # Anthropic Messages API client (ureq + serde)
    prompt.rs          # Prompt construction: spec sections + contract + facts -> prompt text
    compare.rs         # Verdict comparison: LLM output vs ground truth
    report.rs          # TAP or structured reporting of ambiguity findings
    fixtures.rs        # Test case loader: .tenor + facts JSON + expected verdicts
```

Alternative: a separate binary crate alongside the elaborator. This would avoid touching `main.rs` but adds workspace complexity that Phase 2 will handle anyway. For now, adding a subcommand or a separate `[[bin]]` target in the same crate is simpler.

### Pattern 1: Ground Truth from Conformance Suite
**What:** Each ambiguity test case is derived from an existing conformance fixture plus a synthetic fact set. The ground truth for "what verdicts should be produced" comes from the elaborator's expected JSON outputs, not from a separate oracle.
**When to use:** For every test case.
**Example:**
```
Test case structure:
  input:
    spec_excerpt:  relevant sections of docs/TENOR.md
    contract:      conformance/positive/rule_basic.tenor (or similar)
    facts:         { "is_active": true, "balance": {"amount": "500.00", "currency": "USD"}, ... }
  expected:
    verdicts:      ["account_active", "within_credit_limit", ...]  (derived from rule evaluation)
  actual (from LLM):
    verdicts:      ["account_active", "within_credit_limit", ...]
  result:
    match = true  -> spec is clear on this case
    match = false -> AMBIGUITY SIGNAL: spec wording led LLM to different conclusion
```

### Pattern 2: Structured Output for Reliable Parsing
**What:** Use Anthropic's structured outputs beta (or JSON mode via system prompt) to guarantee the LLM returns a parseable JSON response conforming to a specific schema.
**When to use:** Every API call.
**Example:**
```rust
// Request body includes output schema
let request = serde_json::json!({
    "model": "claude-sonnet-4-5-20250514",
    "max_tokens": 4096,
    "system": SYSTEM_PROMPT,
    "messages": [{"role": "user", "content": prompt}],
});
// Parse response, extract content[0].text, deserialize to VerdictResponse
```

The response schema should be:
```json
{
  "verdicts_produced": ["verdict_id_1", "verdict_id_2"],
  "reasoning": "step-by-step evaluation trace",
  "confidence": "high|medium|low",
  "ambiguities_noted": ["description of unclear spec areas"]
}
```

### Pattern 3: Prompt Layering (Spec + Contract + Facts + Question)
**What:** The prompt is constructed in layers: (1) relevant spec sections as system context, (2) the contract source, (3) the fact values, (4) a specific evaluation question.
**When to use:** Every API call.
**Example prompt structure:**
```
SYSTEM: You are a Tenor specification evaluator. Given the spec sections below,
a Tenor contract, and a set of fact values, determine which verdicts are produced
by evaluating the contract's rules against the facts. Follow the spec EXACTLY.

[Spec sections: PredicateExpression evaluation (Section 10), Rule evaluation
(Section 7.3-7.5), relevant BaseType definitions (Section 4)]

USER:
## Contract
[contents of .tenor file]

## Fact Values
[JSON object mapping fact_id -> concrete value]

## Question
Evaluate all rules in this contract against the provided facts.
For each rule, determine whether its `when` condition is satisfied.
If satisfied, the rule produces its declared verdict.
Return the complete set of produced verdicts.
```

### Anti-Patterns to Avoid
- **Sending the entire spec as context:** The spec is ~1500 lines. Sending all of it wastes tokens and dilutes the signal. Send only the sections relevant to the evaluation task (Sections 7, 10, 4, and the relevant construct definitions).
- **Asking the LLM to elaborate (parse) DSL:** The harness tests spec *interpretation*, not parsing ability. Send the contract source for context but ask about evaluation semantics, not syntax.
- **Binary pass/fail without reasoning:** Always require the LLM to show its reasoning. The reasoning trace is where ambiguity signals actually surface -- a wrong answer with correct reasoning points to a spec gap; a wrong answer with wrong reasoning points to an LLM limitation (not a spec bug).
- **Testing negative conformance cases:** Negative tests (expected errors) test elaborator error detection, not evaluation semantics. The ambiguity harness should focus on positive tests where evaluation verdicts can be compared.
- **Ignoring rate limits:** Anthropic API has rate limits. The harness must handle 429 responses with backoff. Running all 47 tests in rapid succession will hit limits.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| HTTP client | Custom socket/TLS code | `ureq` | TLS, connection pooling, timeouts, redirects are all handled |
| JSON serialization | Manual string building | `serde_json` | Already in project; type-safe; handles escaping |
| API response parsing | Regex extraction | Structured outputs or `serde_json::from_str` | LLM output is messy; structured outputs guarantee schema compliance |
| Retry logic with backoff | Manual sleep loops | Simple exponential backoff helper (10-20 lines) | Rate limits (429) and transient errors (500, 503) need retries; a tiny helper is sufficient |
| Diff algorithm for verdicts | Complex tree diff | Set comparison (symmetric difference) | Verdicts are a flat set of IDs; set diff is the right abstraction |

**Key insight:** The Anthropic Messages API is a single POST endpoint with 3 required headers. There is no reason to pull in a full SDK. The entire API client is ~80 lines of Rust with ureq + serde.

## Common Pitfalls

### Pitfall 1: Conflating LLM Errors with Spec Ambiguities
**What goes wrong:** The LLM gets an arithmetic comparison wrong (e.g., Money comparison precision), and the team treats it as a spec ambiguity when it's actually an LLM reasoning limitation.
**Why it happens:** LLMs are imperfect at numeric reasoning, especially with fixed-point decimal arithmetic.
**How to avoid:** Require step-by-step reasoning in the LLM response. Classify disagreements into categories: (a) LLM misunderstood spec text -> spec ambiguity, (b) LLM made arithmetic error -> LLM limitation, (c) LLM hallucinated a rule -> LLM limitation. Only category (a) is actionable.
**Warning signs:** Disagreements cluster around numeric comparisons or complex nested quantifiers.

### Pitfall 2: Non-Deterministic Test Results
**What goes wrong:** The same test case produces different LLM verdicts on different runs, making CI unreliable.
**Why it happens:** LLM outputs are inherently stochastic even at temperature 0 (due to batching and hardware non-determinism).
**How to avoid:** Set `temperature: 0` to minimize variance. Run each test case N times (e.g., 3) and report consensus. A test that produces inconsistent results across runs is a different signal (spec is genuinely ambiguous even to a careful reader) versus a test that consistently disagrees (LLM has a specific misinterpretation of the spec text).
**Warning signs:** Flaky CI results; same test passes and fails across runs.

### Pitfall 3: API Key Management in CI
**What goes wrong:** API key committed to repo, leaked in logs, or missing in CI environment.
**Why it happens:** Developers hard-code keys during development.
**How to avoid:** Read API key from `ANTHROPIC_API_KEY` environment variable only. If missing, skip ambiguity tests with a clear message (not a failure). Never log the key. Use CI secrets management.
**Warning signs:** Tests fail in CI with "missing API key" errors; key appears in git history.

### Pitfall 4: Prompt Too Large for Context Window
**What goes wrong:** The full spec + a complex contract exceeds the model's context window or produces degraded results due to "lost in the middle" effects.
**Why it happens:** The Tenor spec is ~25K tokens; a complex contract adds 2-5K; facts add 1K. Total can approach 30K+ input tokens.
**How to avoid:** Send only the spec sections relevant to the specific test. For rule evaluation tests, send Sections 7 (Rules), 10 (PredicateExpression), 4 (BaseType), and 12 (NumericModel). For flow tests, add Section 11 (Flow). Never send Sections 15-20 (analysis, executor obligations, appendices).
**Warning signs:** LLM responses become vague or miss details from later sections of the prompt.

### Pitfall 5: Treating This as a Conformance Suite
**What goes wrong:** Team expects 100% agreement between LLM and ground truth and treats any disagreement as a bug.
**Why it happens:** Confusion between "conformance testing" (is the implementation correct?) and "ambiguity testing" (is the spec clear?).
**How to avoid:** Frame the output as an ambiguity report, not a pass/fail suite. Every disagreement is investigated -- some are spec ambiguities (actionable), some are LLM limitations (expected). The metric is "number of actionable ambiguities found" not "pass rate."
**Warning signs:** Team spends time trying to make the LLM agree with ground truth instead of investigating why it disagrees.

### Pitfall 6: No Async Runtime Constraint
**What goes wrong:** Pulling in `reqwest` or an async SDK introduces tokio as a transitive dependency, violating the project's "no async runtime in toolchain" constraint.
**Why it happens:** Most Rust HTTP crates default to async.
**How to avoid:** Use `ureq` which is purely synchronous. Verify with `cargo tree` that no tokio dependency is introduced.
**Warning signs:** `cargo tree | grep tokio` shows results after adding HTTP dependency.

## Code Examples

### Anthropic Messages API Call with ureq
```rust
// Source: Anthropic API docs (https://platform.claude.com/docs/en/api/getting-started)
use serde::{Deserialize, Serialize};

const ANTHROPIC_API_URL: &str = "https://api.anthropic.com/v1/messages";
const ANTHROPIC_VERSION: &str = "2023-06-01";

#[derive(Serialize)]
struct Message {
    role: String,
    content: String,
}

#[derive(Serialize)]
struct MessagesRequest {
    model: String,
    max_tokens: u32,
    system: String,
    messages: Vec<Message>,
}

#[derive(Deserialize)]
struct ContentBlock {
    #[serde(rename = "type")]
    content_type: String,
    text: String,
}

#[derive(Deserialize)]
struct MessagesResponse {
    content: Vec<ContentBlock>,
    stop_reason: String,
}

fn call_anthropic(api_key: &str, system: &str, user_prompt: &str) -> Result<String, String> {
    let request = MessagesRequest {
        model: "claude-sonnet-4-5-20250514".to_string(),
        max_tokens: 4096,
        system: system.to_string(),
        messages: vec![Message {
            role: "user".to_string(),
            content: user_prompt.to_string(),
        }],
    };

    let response = ureq::post(ANTHROPIC_API_URL)
        .header("x-api-key", api_key)
        .header("anthropic-version", ANTHROPIC_VERSION)
        .header("content-type", "application/json")
        .send_json(&request)
        .map_err(|e| format!("API call failed: {e}"))?;

    let body: MessagesResponse = response
        .body_mut()
        .read_json()
        .map_err(|e| format!("Failed to parse response: {e}"))?;

    body.content
        .first()
        .map(|c| c.text.clone())
        .ok_or_else(|| "No content in response".to_string())
}
```

### Verdict Set Comparison
```rust
use std::collections::BTreeSet;

struct AmbiguityResult {
    test_name: String,
    expected_verdicts: BTreeSet<String>,
    llm_verdicts: BTreeSet<String>,
    llm_reasoning: String,
    ambiguities_noted: Vec<String>,
}

impl AmbiguityResult {
    fn is_match(&self) -> bool {
        self.expected_verdicts == self.llm_verdicts
    }

    fn missing_verdicts(&self) -> BTreeSet<&String> {
        self.expected_verdicts.difference(&self.llm_verdicts).collect()
    }

    fn extra_verdicts(&self) -> BTreeSet<&String> {
        self.llm_verdicts.difference(&self.expected_verdicts).collect()
    }
}
```

### Test Case Construction from Conformance Fixtures
```rust
struct AmbiguityTestCase {
    name: String,
    spec_sections: Vec<String>,      // Relevant spec section text
    contract_source: String,          // .tenor file contents
    facts: serde_json::Value,         // Synthetic fact values
    expected_verdicts: Vec<String>,   // Derived from expected JSON
}

fn load_test_cases(conformance_dir: &Path) -> Vec<AmbiguityTestCase> {
    // For each positive conformance test that has rules:
    // 1. Read the .tenor source
    // 2. Read the .expected.json to extract rule verdict_types
    // 3. Construct a synthetic fact set that satisfies the rules
    // 4. The expected verdicts are the verdict_types from all rules
    //    whose conditions would be satisfied by those facts
    todo!()
}
```

### Exponential Backoff Retry
```rust
fn with_retry<T, F: Fn() -> Result<T, String>>(f: F, max_retries: u32) -> Result<T, String> {
    let mut delay_ms = 1000;
    for attempt in 0..=max_retries {
        match f() {
            Ok(v) => return Ok(v),
            Err(e) if attempt < max_retries && is_retryable(&e) => {
                eprintln!("Attempt {}: {}; retrying in {}ms", attempt + 1, e, delay_ms);
                std::thread::sleep(std::time::Duration::from_millis(delay_ms));
                delay_ms *= 2;
            }
            Err(e) => return Err(e),
        }
    }
    unreachable!()
}

fn is_retryable(error: &str) -> bool {
    error.contains("429") || error.contains("500") || error.contains("503")
}
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| No spec testing | LLM-as-spec-reader for ambiguity detection | 2024-2025 | Emerging technique; no standardized tooling exists |
| Manual spec review | Automated multi-model spec probing | 2025 | Research papers on using LLMs for requirements analysis |
| Prompt-only JSON | Structured outputs (schema-guaranteed) | Nov 2025 | Anthropic beta; eliminates JSON parsing failures |

**Deprecated/outdated:**
- None directly applicable. This is a novel application pattern.

## Design Decisions for Planner

### Decision 1: What to Test
The harness should test **rule evaluation** -- given facts, which verdicts are produced? This is the core spec interpretation question. Testing elaboration (parsing + type checking) is not useful because the LLM is not implementing a parser; testing flow orchestration is complex and depends on evaluation being correct first.

Start with:
1. **Stratum 0 rules:** Simple fact comparisons (scalar, enum, money)
2. **Stratum 1 rules:** `verdict_present` references and logical connectives
3. **Bounded quantification:** `forall` and `exists` over list facts
4. **Numeric comparisons:** Money and Decimal precision semantics
5. **Negation:** `not verdict_present` semantics

### Decision 2: Synthetic Fact Sets
The existing conformance suite tests elaboration (DSL -> JSON), not evaluation (facts -> verdicts). The test cases need **synthetic fact sets** -- concrete fact values that exercise each rule's `when` condition. For each positive conformance test with rules, create 2-3 fact sets:
- One that satisfies all rule conditions (all verdicts produced)
- One that satisfies some conditions (subset of verdicts)
- One that satisfies none (empty verdict set)

### Decision 3: Ground Truth Derivation
Since the evaluator does not exist yet, ground truth for "which verdicts are produced given these facts" must be **manually derived** from the spec + contract + facts. This is a small number of cases (the positive conformance suite has ~8 tests with rules, yielding ~24 fact-set test cases). Manual derivation is feasible and ensures the ground truth is spec-authoritative, not implementation-authoritative.

### Decision 4: Model Selection
Use Claude Sonnet (claude-sonnet-4-5-20250514) for cost efficiency. The task is analytical (spec interpretation + logical evaluation), not creative. Sonnet is sufficient and much cheaper than Opus for high-throughput CI use. Consider Haiku for initial development/debugging.

### Decision 5: CI Integration
The ambiguity harness should be **opt-in in CI** (controlled by presence of `ANTHROPIC_API_KEY` env var). When the key is absent, tests are skipped with a message. When present, tests run and produce a TAP-format report. Disagreements are informational (warnings), not failures -- they trigger investigation, not build breakage.

## Open Questions

1. **Structured outputs vs. JSON-in-prompt**
   - What we know: Anthropic's structured outputs beta (Nov 2025) guarantees JSON schema compliance. Alternatively, the system prompt can instruct JSON output with high reliability.
   - What's unclear: Whether the structured outputs beta is stable enough for CI use. The beta header is `anthropic-beta: structured-outputs-2025-11-13`.
   - Recommendation: Start with JSON-in-prompt (simpler, no beta dependency). Add structured outputs later if parsing reliability becomes an issue.

2. **How many spec sections to include per prompt**
   - What we know: The full spec is ~25K tokens. Relevant sections for rule evaluation (Sections 4, 7, 10, 12) are ~5-8K tokens.
   - What's unclear: Whether including more context improves or degrades accuracy.
   - Recommendation: Start with minimal relevant sections. If the LLM misinterprets something that a broader context would have clarified, expand. Track prompt size vs. accuracy.

3. **Multi-model comparison**
   - What we know: The phase description mentions "frontier LLM" (singular). The project's spec was co-designed with Claude, GPT, DeepSeek, and Gemini.
   - What's unclear: Whether multi-model comparison (same prompt to multiple models) would surface more ambiguities.
   - Recommendation: Start with single model (Claude Sonnet). Multi-model is a future enhancement -- each model adds API cost and complexity.

4. **ureq v3 compatibility**
   - What we know: ureq recently released v3 with a different API surface from v2. The code examples in this research use v3 API patterns.
   - What's unclear: Whether v3 is stable enough, or if v2 is more battle-tested.
   - Recommendation: Use v3 (latest). The API is simpler and the crate is actively maintained. Fall back to v2 only if v3 has blocking issues.

## Sources

### Primary (HIGH confidence)
- [Anthropic API Overview](https://platform.claude.com/docs/en/api/getting-started) -- Messages API endpoint, headers, request format, response schema
- [ureq GitHub repository](https://github.com/algesten/ureq) -- v3 API, JSON support, blocking I/O design, dependency minimalism
- Tenor spec `docs/TENOR.md` Sections 7, 10, 14 -- Rule evaluation semantics, PredicateExpression evaluation, complete evaluation model
- Existing conformance suite (`conformance/`) and runner (`elaborator/src/runner.rs`) -- ground truth format, test structure, TAP output

### Secondary (MEDIUM confidence)
- [Anthropic Structured Outputs](https://platform.claude.com/docs/en/build-with-claude/structured-outputs) -- Beta feature for guaranteed JSON schema compliance (Nov 2025)
- [crates.io: anthropic-sdk-rust](https://crates.io/crates/anthropic-sdk-rust) -- Community SDK; evaluated and rejected (async-first, heavy dependencies)
- [reqwest::blocking docs](https://docs.rs/reqwest/latest/reqwest/blocking/index.html) -- Evaluated as alternative; rejected due to tokio transitive dependency

### Tertiary (LOW confidence)
- [LLM requirements engineering survey](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1519437/full) -- General patterns for LLM-based spec analysis; not specific to contract DSLs

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- `ureq` + `serde_json` is well-established; Anthropic API is well-documented; the integration is simple
- Architecture: MEDIUM -- The prompt engineering and verdict comparison patterns are sound but untested for this specific domain. Prompt tuning will be needed.
- Pitfalls: HIGH -- Rate limits, API key management, non-determinism, and LLM reasoning limitations are well-documented challenges
- Ground truth derivation: MEDIUM -- Manual derivation is feasible for ~24 test cases but is error-prone; having a second reviewer for ground truth is advisable

**Research date:** 2026-02-21
**Valid until:** 2026-03-21 (30 days -- API and SDK landscape is stable)
