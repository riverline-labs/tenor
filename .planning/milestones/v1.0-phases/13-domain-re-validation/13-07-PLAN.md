---
phase: 13-domain-re-validation
plan: 07
type: execute
wave: 1
depends_on: []
files_modified:
  - domains/saas/saas_activate.verdicts.json
  - domains/saas/saas_suspend.verdicts.json
  - domains/energy_procurement/rfp_approve.verdicts.json
  - domains/energy_procurement/rfp_reject.verdicts.json
  - domains/energy_procurement/rfp_escalate.verdicts.json
  - domains/trade_finance/lc_present.verdicts.json
  - domains/trade_finance/lc_discrepancy.verdicts.json
autonomous: true
requirements: [DOMN-10, DOMN-13, DOMN-14]
gap_closure: true

must_haves:
  truths:
    - "cargo test --workspace passes with 0 failures"
    - "All 7 previously-failing domain eval conformance tests pass"
    - "Verdict fixture files match the JSON structure produced by run_eval_flow_fixture in crates/eval/tests/conformance.rs"
  artifacts:
    - path: "domains/saas/saas_activate.verdicts.json"
      provides: "SaaS activate verdict fixture in OLD format"
      contains: "flow_outcome"
    - path: "domains/saas/saas_suspend.verdicts.json"
      provides: "SaaS suspend verdict fixture in OLD format"
      contains: "flow_outcome"
    - path: "domains/energy_procurement/rfp_approve.verdicts.json"
      provides: "Energy approve verdict fixture with step_type"
      contains: "step_type"
    - path: "domains/energy_procurement/rfp_reject.verdicts.json"
      provides: "Energy reject verdict fixture with step_type"
      contains: "step_type"
    - path: "domains/energy_procurement/rfp_escalate.verdicts.json"
      provides: "Energy escalate verdict fixture with step_type"
      contains: "step_type"
    - path: "domains/trade_finance/lc_present.verdicts.json"
      provides: "Trade finance present verdict fixture in OLD format"
      contains: "flow_outcome"
    - path: "domains/trade_finance/lc_discrepancy.verdicts.json"
      provides: "Trade finance discrepancy verdict fixture in OLD format"
      contains: "flow_outcome"
  key_links:
    - from: "domains/*/verdicts.json"
      to: "crates/eval/tests/conformance.rs"
      via: "run_eval_flow_fixture JSON comparison"
      pattern: "flow_outcome.*step_type.*verdicts"
---

<objective>
Fix 7 failing domain eval conformance tests by reverting verdict fixture files to the format expected by the Rust test harness (run_eval_flow_fixture in crates/eval/tests/conformance.rs).

Purpose: The CLAUDE.md quality gate requires `cargo test --workspace` to pass before any commit. Plans 01, 04, and 05 changed verdict files to a format incompatible with the Rust test runner, introducing 7 failures. This plan restores compatibility.

Output: All 7 verdict fixture files aligned with the Rust test harness output format; `cargo test --workspace` passes with 0 failures.
</objective>

<execution_context>
@/Users/bwb/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bwb/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-domain-re-validation/13-VERIFICATION.md
@crates/eval/tests/conformance.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Regenerate all 7 verdict fixture files to match Rust test harness format</name>
  <files>
    domains/saas/saas_activate.verdicts.json
    domains/saas/saas_suspend.verdicts.json
    domains/energy_procurement/rfp_approve.verdicts.json
    domains/energy_procurement/rfp_reject.verdicts.json
    domains/energy_procurement/rfp_escalate.verdicts.json
    domains/trade_finance/lc_present.verdicts.json
    domains/trade_finance/lc_discrepancy.verdicts.json
  </files>
  <action>
The Rust test harness `run_eval_flow_fixture` (conformance.rs lines 120-132) builds JSON with this exact structure:

```json
{
  "verdicts": [/* flat array from result.verdicts.to_json()["verdicts"] */],
  "flow_outcome": "/* from result.flow_result.outcome */",
  "steps_executed": [
    {
      "step_id": "...",
      "step_type": "...",
      "result": "..."
    }
  ]
}
```

The correct approach is to run each scenario through the evaluator and capture the actual output to create correct fixture files. For each of the 7 fixtures:

1. Run the flow evaluation using the Rust evaluator (via `cargo test` with RUST_LOG or by writing a small script)
2. Capture the actual JSON the test harness produces (the "left" side of the assert_eq)
3. Write that exact JSON to the `.verdicts.json` file

Alternatively, manually transform each file to match:

**SaaS files (saas_activate, saas_suspend) -- currently in NEW CLI format:**
- Change `"outcome"` key to `"flow_outcome"`
- Remove `"entity_state_changes"`, `"flow_id"`, `"initiating_persona"` keys
- Change `"verdicts": {"verdicts": [...]}` (nested) to `"verdicts": [...]` (flat array)
- Add `"step_type"` to each step in `steps_executed` (the Rust harness emits step_type for every step)

**Energy files (rfp_approve, rfp_reject, rfp_escalate) -- partially transformed (have flow_outcome, flat verdicts, but missing step_type):**
- Add `"step_type"` field to each step in `steps_executed`
- step_type values are determined by the evaluator based on the step kind in the contract

**Trade finance files (lc_present, lc_discrepancy) -- currently in NEW CLI format:**
- Change `"outcome"` key to `"flow_outcome"`
- Remove `"entity_state_changes"`, `"flow_id"`, `"initiating_persona"` keys
- Change `"verdicts": {"verdicts": [...]}` (nested) to `"verdicts": [...]` (flat array)
- Add `"step_type"` to each step in `steps_executed`

The easiest and most reliable method: run `cargo test -p tenor-eval --test conformance domain_saas_activate 2>&1` for each failing test. The test output shows "Actual:" which is the JSON the harness generates -- that IS the correct format. Copy the "Actual:" JSON into each verdict file.

Reference the working fixtures for format validation: `domains/healthcare/prior_auth_approve.verdicts.json` and `domains/supply_chain/inspection_pass.verdicts.json` both use the OLD format with `flow_outcome`, `step_type`, and flat `verdicts` array.

All JSON keys must be sorted lexicographically within each object (per CLAUDE.md serialization rules).
  </action>
  <verify>
Run all quality gates in order:
```
cargo fmt --all
cargo build --workspace
cargo test --workspace
cargo run -p tenor-cli -- test conformance
cargo clippy --workspace -- -D warnings
```
All 5 checks must pass. Specifically verify the 7 previously-failing tests now pass:
```
cargo test -p tenor-eval --test conformance domain_saas_activate domain_saas_suspend domain_energy_approve domain_energy_reject domain_energy_escalate domain_trade_finance_present domain_trade_finance_discrepancy
```
  </verify>
  <done>
All 7 domain eval conformance tests pass. `cargo test --workspace` reports 0 failures. The CLAUDE.md mandatory pre-commit quality gate is satisfied. All verdict fixture files use the OLD format matching `run_eval_flow_fixture` output: `flow_outcome` (not `outcome`), `step_type` in each step, flat `verdicts` array (not nested).
  </done>
</task>

</tasks>

<verification>
1. `cargo test --workspace` -- 0 failures (was 7 failures before)
2. `cargo run -p tenor-cli -- test conformance` -- 72/72 pass (no regressions)
3. Each of the 7 verdict files contains `"flow_outcome"` (not `"outcome"`)
4. Each of the 7 verdict files contains `"step_type"` in steps_executed entries
5. None of the 7 verdict files contains `"entity_state_changes"`, `"flow_id"`, or `"initiating_persona"`
6. `cargo clippy --workspace -- -D warnings` -- 0 warnings
</verification>

<success_criteria>
- `cargo test --workspace` exits 0 with all tests passing
- The 7 specific domain eval tests (domain_saas_activate, domain_saas_suspend, domain_energy_approve, domain_energy_reject, domain_energy_escalate, domain_trade_finance_present, domain_trade_finance_discrepancy) all pass
- No regressions introduced (72/72 conformance tests, all other workspace tests)
- CLAUDE.md pre-commit quality gates fully satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/13-domain-re-validation/13-07-SUMMARY.md`
</output>
